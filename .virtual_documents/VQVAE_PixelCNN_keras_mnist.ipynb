import tensorflow as tf
from tensorflow.keras import layers, Model
import numpy as np
import matplotlib.pyplot as plt


# Load MNIST dataset
(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()
train_images = (train_images.astype("float32") / 255.0).reshape(-1, 28, 28, 1)
test_images = (test_images.astype("float32") / 255.0).reshape(-1, 28, 28, 1)

print("Training images shape:", train_images.shape)  
print("Testing images shape:", test_images.shape)


data_variance = np.var(train_images)
data_variance


# Parameters
embedding_dim = 16  # Dimensionality of the embedding space
num_embeddings = 128  # Number of embedding vectors
beta = 0.25  # Commitment loss factor


# VQ-VAE Encoder
class Encoder(Model):
    def __init__(self):
        super(Encoder, self).__init__()
        self.conv1 = layers.Conv2D(32, 3, strides=2, padding='same')
        self.conv2 = layers.Conv2D(64, 3, strides=2, padding='same')
        self.conv3 = layers.Conv2D(16, 1, strides=1, padding='same')

    def call(self, x):
        x = tf.nn.relu(self.conv1(x))
        x = tf.nn.relu(self.conv2(x))
        x = self.conv3(x)
        return x


# VQ-VAE Decoder
class Decoder(Model):
    def __init__(self):
        super(Decoder, self).__init__()
        self.conv1 = layers.Conv2DTranspose(64, 3, strides=2, padding='same')
        self.conv2 = layers.Conv2DTranspose(32, 3, strides=2, padding='same')
        self.conv3 = layers.Conv2DTranspose(1, 3, strides=1, padding='same')

    def call(self, x):
        x = tf.nn.relu(self.conv1(x))
        x = tf.nn.relu(self.conv2(x))
        x = self.conv3(x)
        return x


class VectorQuantizer(layers.Layer):
    def __init__(self, num_embeddings, embedding_dim, beta):
        super(VectorQuantizer, self).__init__()
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.beta = beta
        self.embeddings = self.add_weight(shape=(num_embeddings, embedding_dim), initializer='uniform', trainable=True)

    def call(self, x):
        flat_x = tf.reshape(x, [-1, self.embedding_dim])
        distances = (tf.reduce_sum(flat_x ** 2, axis=1, keepdims=True) 
                     - 2 * tf.matmul(flat_x, self.embeddings, transpose_b=True)
                     + tf.reduce_sum(self.embeddings ** 2, axis=1)) #-----------------------------> Try axis=0
        encoding_indices = tf.argmin(distances, axis=1)
        encodings = tf.one_hot(encoding_indices, self.num_embeddings)
        quantized = tf.matmul(encodings, self.embeddings)
        quantized = tf.reshape(quantized, tf.shape(x))

        # Loss for commitment to the codebook
        commitment_loss = self.beta * tf.reduce_mean((tf.stop_gradient(quantized) - x) ** 2)
        #self.add_loss(commitment_loss)
        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)
        self.add_loss(self.beta * commitment_loss + codebook_loss)

        quantized = x + tf.stop_gradient(quantized - x)
        return quantized, encodings, encoding_indices


class VQVAE(Model):
    def __init__(self, embedding_dim, num_embeddings, beta):
        super(VQVAE, self).__init__()
        self.encoder = Encoder()
        self.vector_quantizer = VectorQuantizer(num_embeddings, embedding_dim, beta)
        self.decoder = Decoder()

    def call(self, x):
        z_e = self.encoder(x)
        quantized, encodings, encoding_indices = self.vector_quantizer(z_e)
        x_recon = self.decoder(quantized)
        return x_recon, encodings, encoding_indices


vqvae = VQVAE(embedding_dim, num_embeddings, data_variance)
optimizer = tf.keras.optimizers.Adam()


# Prepare dataset
batch_size = 128
train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(100).batch(batch_size)
test_dataset = tf.data.Dataset.from_tensor_slices(test_images).batch(batch_size)


# Training VQ-VAE
def train_vqvae(dataset, epochs):
    for epoch in range(epochs):
        for step, x_batch in enumerate(dataset):
            with tf.GradientTape() as tape:
                recon_images, _, _ = vqvae(x_batch)
                loss = tf.reduce_mean((x_batch - recon_images) ** 2) + sum(vqvae.losses)

            grads = tape.gradient(loss, vqvae.trainable_variables)
            optimizer.apply_gradients(zip(grads, vqvae.trainable_variables))

        print(f"Epoch {epoch + 1}, Loss: {loss.numpy()}")


# Train VQ-VAE
train_vqvae(train_dataset, epochs=30)


def show_subplot(original, reconstructed):
    plt.subplot(1, 2, 1)
    plt.imshow(original.squeeze() + 0.5)
    plt.title("Original")
    plt.axis("off")

    plt.subplot(1, 2, 2)
    plt.imshow(reconstructed.squeeze() + 0.5)
    plt.title("Reconstructed")
    plt.axis("off")

    plt.show()


idx = np.random.choice(len(test_images), 10)
test_images = test_images[idx]
reconstructions_test = vqvae.predict(test_images)

for test_image, reconstructed_image in zip(test_images, reconstructions_test[0]):
    show_subplot(test_image, reconstructed_image)


encoder = vqvae.encoder
quantizer = vqvae.vector_quantizer

encoded_outputs = encoder.predict(test_images)
flat_enc_outputs = encoded_outputs.reshape(-1, encoded_outputs.shape[-1])
_, _, codebook_indices = quantizer(flat_enc_outputs)
codebook_indices = codebook_indices.numpy().reshape(encoded_outputs.shape[:-1])

for i in range(len(test_images)):
    plt.subplot(1, 2, 1)
    plt.imshow(test_images[i].squeeze() + 0.5)
    plt.title("Original")
    plt.axis("off")

    plt.subplot(1, 2, 2)
    plt.imshow(codebook_indices[i])
    plt.title("Code")
    plt.axis("off")
    plt.show()


def get_encoding_indices(model, dataset):
    encoding_indices = []
    for x_batch in dataset:
        _, _, batch_indices = model(x_batch)
        #encoding_indices.append(batch_indices.numpy())
        encoding_indices.append(batch_indices.numpy().reshape((-1, 7, 7)))
    return np.concatenate(encoding_indices, axis=0)

train_encoding_indices = get_encoding_indices(vqvae, train_dataset)
test_encoding_indices = get_encoding_indices(vqvae, test_dataset)





print("Training encoding indices shape:", train_encoding_indices.shape)  
print("Testing encoding indices shape:", test_encoding_indices.shape)


num_residual_blocks = 2
num_pixelcnn_layers = 2
pixelcnn_input_shape = (7, 7)
print(f"Input shape of the PixelCNN: {pixelcnn_input_shape}")


# The first layer is the PixelCNN layer. This layer simply
# builds on the 2D convolutional layer, but includes masking.
class PixelConvLayer(layers.Layer):
    def __init__(self, mask_type, **kwargs):
        super().__init__()
        self.mask_type = mask_type
        self.conv = layers.Conv2D(**kwargs)

    def build(self, input_shape):
        # Build the conv2d layer to initialize kernel variables
        self.conv.build(input_shape)
        # Use the initialized kernel to create the mask
        kernel_shape = self.conv.kernel.shape
        self.mask = np.zeros(shape=kernel_shape)
        self.mask[: kernel_shape[0] // 2, ...] = 1.0
        self.mask[kernel_shape[0] // 2, : kernel_shape[1] // 2, ...] = 1.0
        if self.mask_type == "B":
            self.mask[kernel_shape[0] // 2, kernel_shape[1] // 2, ...] = 1.0

    def call(self, inputs):
        self.conv.kernel.assign(self.conv.kernel * self.mask)
        return self.conv(inputs)


# Next, we build our residual block layer.
# This is just a normal residual block, but based on the PixelConvLayer.
class ResidualBlock(layers.Layer):
    def __init__(self, filters, **kwargs):
        super().__init__(**kwargs)
        self.conv1 = tf.keras.layers.Conv2D(
            filters=filters, kernel_size=1, activation="relu"
        )
        self.pixel_conv = PixelConvLayer(
            mask_type="B",
            filters=filters // 2,
            kernel_size=3,
            activation="relu",
            padding="same",
        )
        self.conv2 = tf.keras.layers.Conv2D(
            filters=filters, kernel_size=1, activation="relu"
        )

    def call(self, inputs):
        x = self.conv1(inputs)
        x = self.pixel_conv(x)
        x = self.conv2(x)
        return layers.add([inputs, x])


class OneHotLayer(tf.keras.layers.Layer):
    def __init__(self, num_embeddings, **kwargs):
        super(OneHotLayer, self).__init__(**kwargs)
        self.num_embeddings = num_embeddings

    def call(self, inputs):
        # One-hot encode the input tensor
        return tf.one_hot(inputs, self.num_embeddings)


pixelcnn_inputs = tf.keras.Input(shape=pixelcnn_input_shape, dtype=tf.int32)
#ohe = tf.one_hot(pixelcnn_inputs, num_embeddings)
ohe = OneHotLayer(num_embeddings=num_embeddings)(pixelcnn_inputs)

x = PixelConvLayer(
    mask_type="A", filters=128, kernel_size=7, activation="relu", padding="same"
)(ohe)

for _ in range(num_residual_blocks):
    x = ResidualBlock(filters=128)(x)

for _ in range(num_pixelcnn_layers):
    x = PixelConvLayer(
        mask_type="B",
        filters=128,
        kernel_size=1,
        strides=1,
        activation="relu",
        padding="valid",
    )(x)

out = tf.keras.layers.Conv2D(
    filters=num_embeddings, kernel_size=1, strides=1, padding="valid"
)(x)

pixel_cnn = tf.keras.Model(pixelcnn_inputs, out, name="pixel_cnn")
pixel_cnn.summary()


pixel_cnn.compile(
    optimizer=tf.keras.optimizers.Adam(3e-4),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)


pixel_cnn.fit(
    x=train_encoding_indices,
    y=train_encoding_indices,
    batch_size=128,
    epochs=30,
    validation_split=0.1,
)


import tensorflow_probability as tfp

class CategoricalDistributionLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(CategoricalDistributionLayer, self).__init__(**kwargs)

    def call(self, logits):
        # Create a categorical distribution from logits
        distribution = tfp.distributions.Categorical(logits=logits)
        # Sample from the distribution to get the output
        return distribution.sample()  # Ensure to return a tensor

    def get_config(self):
        config = super(CategoricalDistributionLayer, self).get_config()
        return config

iinputs = tf.keras.Input(shape=(None, 7, 7, 128))  # Adjust the shape as needed
outputs = pixel_cnn(inputs, training=False)

# Use the categorical distribution layer
categorical_layer = CategoricalDistributionLayer()
outputs = categorical_layer(outputs)

# Create the model
sampler = tf.keras.Model(inputs, outputs)
sampler.summary()


# Create an empty array of priors.
batch = 10
priors = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:])
batch, rows, cols = priors.shape

# Iterate over the priors because generation has to be done sequentially pixel by pixel.
for row in range(rows):
    for col in range(cols):
        # Feed the whole array and retrieving the pixel value probabilities for the next
        # pixel.
        probs = sampler.predict(priors)
        # Use the probabilities to pick pixel values and append the values to the priors.
        priors[:, row, col] = probs[:, row, col]

print(f"Prior shape: {priors.shape}")


# Perform an embedding lookup.
pretrained_embeddings = quantizer.embeddings
priors_ohe = tf.one_hot(priors.astype("int32"), num_embeddings).numpy()
priors_ohe_reshaped = tf.reshape(priors_ohe, (-1, 128))
quantized = tf.matmul(priors_ohe_reshaped, pretrained_embeddings) 
#quantized = tf.reshape(quantized, (-1, *(encoded_outputs.shape[1:])))
quantized = tf.reshape(quantized, (-1, 7, 7, 16)) 

# Generate novel images.
decoder = vqvae.decoder
generated_samples = decoder.predict(quantized)

for i in range(batch):
    plt.subplot(1, 2, 1)
    plt.imshow(priors[i])
    plt.title("Code")
    plt.axis("off")

    plt.subplot(1, 2, 2)
    plt.imshow(generated_samples[i].squeeze() + 0.5)
    plt.title("Generated Sample")
    plt.axis("off")
    plt.show()



